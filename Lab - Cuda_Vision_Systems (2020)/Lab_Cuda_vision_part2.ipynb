{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classification_model_tl.ipynb","provenance":[],"mount_file_id":"1KYI4xiC7Od_Ta509uyvceezuYu2cGspM","authorship_tag":"ABX9TyMSSHOHzNe4zJ6AgRPMqR5f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e6b99af529e34359a0b5e3f6e9d50626":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bcdb8969c5dd45cea48b7cd7fc582d1b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6618cf5042ff44f1987d25726e74c2b7","IPY_MODEL_1889977d151443d196da8b2093d125dc"]}},"bcdb8969c5dd45cea48b7cd7fc582d1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"6618cf5042ff44f1987d25726e74c2b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0bdb0206be2e4f55a2dad5318f4a66ef","_dom_classes":[],"description":"Validation sanity check: 100%","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_86fb6cab96904b09a41e9686b7cd10cb"}},"1889977d151443d196da8b2093d125dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cbb2c4e20c734057b9abf082467ade30","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1.0 [02:21&lt;00:00, 135.86s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e7ba5686a630441488f2b0c7ee6a5cb6"}},"0bdb0206be2e4f55a2dad5318f4a66ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"86fb6cab96904b09a41e9686b7cd10cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cbb2c4e20c734057b9abf082467ade30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e7ba5686a630441488f2b0c7ee6a5cb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b503583aa4a4094af72faa1bb00600a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7e413ef65e1241339f9c3e828fa744c4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ebd2de4296d9460c8580b7d8abd53180","IPY_MODEL_455f7602259249bda031984ab512fdc4"]}},"7e413ef65e1241339f9c3e828fa744c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"ebd2de4296d9460c8580b7d8abd53180":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c372e6bec6b2493da7c29d30cd15d055","_dom_classes":[],"description":"Epoch 0:   0%","_model_name":"FloatProgressModel","bar_style":"info","max":666,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6af9c196021e46e7be6da9b82af41780"}},"455f7602259249bda031984ab512fdc4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cf43a1a7d2054876bfbe414e016da892","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/666 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4eaae2f537bc4edeab21e3b471a453ca"}},"c372e6bec6b2493da7c29d30cd15d055":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6af9c196021e46e7be6da9b82af41780":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf43a1a7d2054876bfbe414e016da892":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4eaae2f537bc4edeab21e3b471a453ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"PTZcnNRKf6PW","colab_type":"code","colab":{}},"source":["\"\"\"\n","  run and restart runtime once, av library needs that for some reason, it is need \n","\"\"\"\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","!pip install pytorch_lightning\n","import torch.utils as torchU\n","import pytorch_lightning as torch_light\n","from torchvision import datasets, models, transforms\n","import numpy as np\n","import os\n","from matplotlib import pyplot as plt\n","import math\n","from torch.autograd import Variable\n","!pip install av\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XA5fB4mC0dSH","colab":{}},"source":["%cd \"/content/drive/My Drive/Cuda Vision Lab Final Project\"\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GoEJDqhymsUl","colab_type":"code","colab":{}},"source":["%cd \"/content\"\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtK7rvq3DhC-","colab_type":"code","colab":{}},"source":["\"\"\"\n","  copy pasted from pytorch_ssim for easier code control and ability to change\n","\"\"\"\n","def gaussian(window_size, sigma):\n","    gauss = torch.Tensor([math.exp(-(x - window_size/2)**2/float(2*sigma**2)) for x in range(window_size)])\n","    return gauss/gauss.sum()\n","\n","def create_window(window_size, channel):\n","    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n","    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n","    window = Variable(_2D_window.expand(channel, 1, window_size, window_size))\n","    return window\n","\n","def _ssim(img1, img2, window, window_size, channel, size_average = True):\n","    int_pad = int(window_size/2)\n","    mu1 = F.conv2d(img1, window, padding = int_pad, groups = channel)\n","    mu2 = F.conv2d(img2, window, padding = int_pad, groups = channel)\n","\n","    mu1_sq = mu1.pow(2)\n","    mu2_sq = mu2.pow(2)\n","    mu1_mu2 = mu1*mu2\n","\n","    int_pad = int(window_size/2)\n","    sigma1_sq = F.conv2d(img1*img1, window, padding = int_pad, groups = channel) - mu1_sq\n","    sigma2_sq = F.conv2d(img2*img2, window, padding = int_pad, groups = channel) - mu2_sq\n","    sigma12 = F.conv2d(img1*img2, window, padding = int_pad, groups = channel) - mu1_mu2\n","\n","    C1 = 0.01**2\n","    C2 = 0.03**2\n","\n","    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n","\n","    if size_average:\n","        return ssim_map.mean()\n","    else:\n","        return ssim_map.mean(1).mean(1).mean(1)\n","\n","class SSIM(torch.nn.Module):\n","    def __init__(self, window_size = 11, size_average = True):\n","        super(SSIM, self).__init__()\n","        self.window_size = window_size\n","        self.size_average = size_average\n","        self.channel = 1\n","        self.window = create_window(window_size, self.channel)\n","\n","    def forward(self, img1, img2):\n","        (_, channel, _, _) = img1.size()\n","\n","        if channel == self.channel:\n","            window = self.window\n","        else:\n","            window = create_window(self.window_size, channel)\n","            self.window = window\n","            self.channel = channel\n","\n","        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n","\n","def ssim(img1, img2, window_size = 11, size_average = True):\n","    (_, channel, _, _) = img1.size()\n","    window = create_window(window_size, channel)\n","    return _ssim(img1, img2, window, window_size, channel, size_average)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YtQvcJ0P7ogS","colab_type":"code","colab":{}},"source":["\"\"\"\n","  code is taken from following Git-repository:\n","  https://github.com/happyjin/ConvGRU-pytorch.git\n","  ConvLSTM_pytorch is released under the MIT License (refer to the LICENSE file for details).\n","\"\"\"\n","class ConvGRU(nn.Module):\n","  def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias, dtype, batch_size):\n","    \"\"\"\n","    Initialize the ConvLSTM cell\n","    :param input_size: (int, int)\n","        Height and width of input tensor as (height, width).\n","    :param input_dim: int\n","        Number of channels of input tensor.\n","    :param hidden_dim: int\n","        Number of channels of hidden state.\n","    :param kernel_size: (int, int)\n","        Size of the convolutional kernel.\n","    :param bias: bool\n","        Whether or not to add the bias.\n","    :param dtype: torch.cuda.FloatTensor or torch.FloatTensor\n","        Whether or not to use cuda.\n","    \"\"\"\n","    super(ConvGRU, self).__init__()\n","    self.height, self.width = input_size\n","    self.padding = kernel_size // 2, kernel_size // 2\n","    self.hidden_dim = hidden_dim\n","    self.bias = bias\n","    self.dtype = dtype\n","    self.init_hidden(batch_size)\n","\n","    self.conv_gates = nn.Conv2d(in_channels=input_dim + hidden_dim,\n","                                out_channels=2*self.hidden_dim,  # for update_gate,reset_gate respectively\n","                                kernel_size=kernel_size,\n","                                padding=self.padding,\n","                                bias=self.bias)\n","\n","    self.conv_can = nn.Conv2d(in_channels=input_dim+hidden_dim,\n","                          out_channels=self.hidden_dim, # for candidate neural memory\n","                          kernel_size=kernel_size,\n","                          padding=self.padding,\n","                          bias=self.bias)\n","  def init_hidden(self, batch_size):\n","    self.h_cur = (Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).type(self.dtype))\n","  def forward(self, input_tensor):\n","    \"\"\"\n","    :param self:\n","    :param input_tensor: (b, c, h, w)\n","        input is actually the target_model\n","    :param h_cur: (b, c_hidden, h, w)\n","        current hidden and cell states respectively\n","    :return: h_next,\n","        next hidden state\n","    \"\"\"\n","    self.check_device(input_tensor)\n","    combined = torch.cat([input_tensor, self.h_cur], dim=1)\n","    combined_conv = self.conv_gates(combined)\n","\n","    gamma, beta = torch.split(combined_conv, self.hidden_dim, dim=1)\n","    reset_gate = torch.sigmoid(gamma)\n","    update_gate = torch.sigmoid(beta)\n","\n","    combined = torch.cat([input_tensor, reset_gate*self.h_cur], dim=1)\n","    cc_cnm = self.conv_can(combined)\n","    cnm = torch.tanh(cc_cnm)\n","\n","    h_next = (1 - update_gate) * self.h_cur + update_gate * cnm\n","    return h_next\n","  def check_device(self,input_tensors):\n","    if self.h_cur.device != input_tensors.device:\n","      self.h_cur.data = self.h_cur.to(input_tensors.get_device())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfbAMCqxZ3gW","colab_type":"code","colab":{}},"source":["\"\"\"\n","  This module is provided from our supervisor Hafez Farazi\n","  related paper: http://ais.uni-bonn.de/~hfarazi/papers/LocDep.pdf\n","  code also provided from: https://github.com/AIS-Bonn/LocDepVideoPrediction.git\n","\"\"\"\n","class LocationAwareConv2d(torch.nn.Conv2d):\n","    def __init__(self,w,h,in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n","        super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n","        self.locationBias=torch.nn.Parameter(torch.zeros(w,h,3))\n","        self.locationEncode=torch.autograd.Variable(torch.ones(w,h,3))\n","    def forward(self,inputs):\n","        if self.locationBias.device != inputs.device:\n","            self.locationBias.data=self.locationBias.to(inputs.get_device())\n","        if self.locationEncode.device != inputs.device:\n","            self.locationEncode.data=self.locationEncode.to(inputs.get_device())\n","        b=self.locationBias*self.locationEncode\n","        return super().forward(inputs)+b[:,:,0]+b[:,:,1]+b[:,:,2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"glXn3MtlTGC_","colab_type":"code","colab":{}},"source":["\"\"\"\n","  module to deal with midblocks of model 1\n","\"\"\"\n","class Paraleller(nn.Module):\n","  def __init__(self,list_of_modules):\n","    super(Paraleller, self).__init__()\n","    self.module_list = nn.ModuleList(list_of_modules)\n","  def forward(self, x):\n","    ans = []\n","    for module in self.module_list:\n","      ans.append(module(x))\n","    return ans"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vB2BBtQZSAUp","colab_type":"code","colab":{}},"source":["\"\"\"\n","  download the dataset and annonations\n","\"\"\"\n","!wget https://www.crcv.ucf.edu/data/UCF101/UCF101.rar --no-check-certificate\n","!wget https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip --no-check-certificate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gt_n7hEqSIQ5","colab_type":"code","colab":{}},"source":["\"\"\"\n","  unpack downloaded dataset and annnonations\n","\"\"\"\n","get_ipython().system_raw(\"unrar x UCF101.rar\")\n","!unzip UCF101TrainTestSplits-RecognitionTask.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7YWq4t37glp","colab_type":"code","colab":{}},"source":["# important constant: (frames_per_clip-3)*batchsize = true batchsize\n","frames_per_clip = 50\n","frames_between_clip = 200\n","inp_dim=64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KYKI5pU-GzA","colab_type":"code","colab":{}},"source":["tfs = transforms.Compose([\n","             # scale in [0, 1]\n","             transforms.Lambda(lambda x: x / 255.),\n","             # reshape into (T, C, H, W)\n","             transforms.Lambda(lambda x: x.permute(0, 3, 1, 2) ) ,\n","             # resize frames\n","             transforms.Lambda(lambda x: F.interpolate(x,(inp_dim,inp_dim)))\n","    ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0IjddAk45kD7","colab_type":"code","colab":{}},"source":["def loader_avi_to_tensor(path):\n","  frames,_,_= torchvision.io.read_video(path)\n","  return frames"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HkeoUtef4X3g","colab_type":"code","colab":{}},"source":["\"\"\"\n","  using generic DatasetLoader\n","\"\"\"\n","ufc101_dset_train = datasets.folder.DatasetFolder(  root=\"UCF-101\",\n","                                                    loader=loader_avi_to_tensor,\n","                                                    extensions=(\"avi\"),\n","                                                    transform=tfs                                             \n","                                                  )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWqmNt3A6Fg8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600265350279,"user_tz":-120,"elapsed":430,"user":{"displayName":"Heiko Schmidt","photoUrl":"","userId":"06882927474289994531"}},"outputId":"e1193b04-de4f-44ea-b273-50b317280c66"},"source":["print(len(ufc101_dset_train))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["13320\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dhTWpvMNfMA7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"executionInfo":{"status":"ok","timestamp":1600266162464,"user_tz":-120,"elapsed":3188,"user":{"displayName":"Heiko Schmidt","photoUrl":"","userId":"06882927474289994531"}},"outputId":"950e4528-d3aa-49a8-c82a-19d364559bb0"},"source":["loader = torchU.data.DataLoader(ufc101_dset_train,batch_size=1,collate_fn=custom_collate)\n","rnd_number = int(torch.randint(10,size=(1,)))\n","print(rnd_number)\n","counter=1\n","chosen_batch = None\n","chosen_labels = None\n","for batches in loader:\n","  if counter==rnd_number:\n","    chosen_batch = batches\n","    break\n","  counter+=1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n","  + \"follow-up version. Please use pts_unit 'sec'.\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"dSO1o6KPmw9G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600266174199,"user_tz":-120,"elapsed":544,"user":{"displayName":"Heiko Schmidt","photoUrl":"","userId":"06882927474289994531"}},"outputId":"b2499619-306f-4971-cc3f-de547a486764"},"source":["for video,label in chosen_batch:\n","  print(video.shape,label)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([216, 3, 64, 64]) 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7CdkuOcEwm8G","colab_type":"code","colab":{}},"source":["def custom_collate(batch):\n","    filtered_batch = []\n","    for video in batch:\n","        filtered_batch.append(video)\n","    return filtered_batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MeF7qisomNru","colab_type":"code","colab":{}},"source":["def calculate_split(dataset_len,batch_size):\n","  len1 = int(dataset_len*0.6)\n","  while len1%batch_size!=0:\n","    len1-=len1%batch_size\n","  if dataset_len-len1%2==0:\n","    len2 = (dataset_len-len1)//2\n","    len3 = len2\n","  else:\n","    len2 = (dataset_len-len1-1)//2\n","    len3 = dataset_len-len1-len2\n","  return len1,len2,len3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uAxqS-Yg8qHo","colab":{}},"source":["class pred_Project(torch_light.LightningModule):\n","  def __init__(self, hparams):\n","    super(pred_Project, self).__init__()\n","    self.hparams = hparams\n","    \n","    \"\"\"\n","      load pretrained resnet18\n","    \"\"\"\n","    self.resnet = models.resnet18(pretrained=True)\n","    \n","    \"\"\"\n","      some constants of the network\n","    \"\"\"\n","    inp_size = self.hparams[\"input_dim\"] #assumption number is even and power of 2\n","    i2 = int(inp_size/2)\n","    i4 = int(inp_size/4)\n","    i8 = int(inp_size/8)\n","    i16 = int(inp_size/16)\n","    true_batchsize = self.hparams[\"batch_size\"]\n","    \"\"\"\n","      create a partial layer of the proposed network and structure them in packs\n","    \"\"\"\n","    #BN+Relu+Maxpool\n","    self.pre_block = nn.Sequential(\n","        self.resnet.bn1,\n","        self.resnet.relu,\n","        self.resnet.maxpool\n","    )\n","    #middle row block 1\n","    self.midblock1 = Paraleller(\n","        [nn.Conv2d(64,64,1,1),\n","          ConvGRU((i2,i2),64,64,kernel_size=3,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n","          ConvGRU((i2,i2),64,64,kernel_size=5,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n","          ConvGRU((i2,i2),64,64,kernel_size=7,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize)  \n","        ]\n","    )\n","    #middle row block 2\n","    self.midblock2 = Paraleller(\n","        [nn.Conv2d(64,64,1,1),\n","          ConvGRU((i4,i4),64,64,kernel_size=3,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n","          ConvGRU((i4,i4),64,64,kernel_size=5,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n","          ConvGRU((i4,i4),64,64,kernel_size=7,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize)\n","        ]\n","    )\n","    #middle row block 3\n","    self.midblock3 = Paraleller(\n","        [LocationAwareConv2d(w=i8,h=i8,in_channels=128,out_channels=64,kernel_size=1),\n","          ConvGRU((i8,i8),128,64,kernel_size=3,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n","          ConvGRU((i8,i8),128,64,kernel_size=5,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n","          ConvGRU((i8,i8),128,64,kernel_size=7,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize)\n","        ]\n","    )\n","    #middle row block 4\n","    self.midblock4 = Paraleller(\n","        [LocationAwareConv2d(w=i16,h=i16,in_channels=256,out_channels=64,kernel_size=1),\n","          ConvGRU((i16,i16),256,64,kernel_size=3,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n","          ConvGRU((i16,i16),256,64,kernel_size=5,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n","          ConvGRU((i16,i16),256,64,kernel_size=7,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize)\n","        ]\n","    )\n","    #left block block 1\n","    self.leftblock1 = nn.Sequential(\n","        nn.ReLU(),\n","        nn.Conv2d(512,1024,3,1,1),\n","        nn.PixelShuffle(2),\n","        nn.Conv2d(256,64,3,1,1)\n","    )\n","    #left block block 2\n","    self.leftblock2 = nn.Sequential(\n","        nn.ReLU(),\n","        nn.Conv2d(320,1024,3,1,1),\n","        nn.PixelShuffle(2),\n","        nn.Conv2d(256,64,3,1,1)\n","    )\n","    #left block block 3\n","    self.leftblock3 = nn.Sequential(\n","        nn.ReLU(),\n","        nn.Conv2d(320,1024,3,1,1),\n","        nn.PixelShuffle(2),\n","        nn.Conv2d(256,64,3,1,1)\n","    )\n","    #left block block 4\n","    self.leftblock4 = nn.Sequential(\n","        nn.ReLU(),\n","        nn.Conv2d(320,1024,3,1,1),\n","        nn.PixelShuffle(2),\n","        nn.Conv2d(256,64,3,1,1)\n","    )\n","    # last layer\n","    self.lucky_layer = nn.Conv2d(320,3,1,1,0)\n","  def forward(self, x):\n","      x = self.resnet.conv1(x)\n","      self.feed_to_midblock1 = x.clone()\n","      x = self.pre_block(x)\n","      x = self.resnet.layer1(x)\n","      self.feed_to_midblock2 = x.clone()\n","      x = self.resnet.layer2(x)\n","      self.feed_to_midblock3 = x.clone()\n","      x = self.resnet.layer3(x)\n","      self.feed_to_midblock4 = x.clone()\n","      x = self.resnet.layer4(x)\n","      \n","      x = self.leftblock1(x)\n","      y4 = self.midblock4(self.feed_to_midblock4)\n","      y4.append(x)\n","      x = torch.cat(y4,dim=1)\n","\n","      x = self.leftblock2(x)\n","      y3 = self.midblock3(self.feed_to_midblock3)\n","      y3.append(x)\n","      x = torch.cat(y3,dim=1)\n","\n","      x=self.leftblock3(x)\n","      y2 = self.midblock2(self.feed_to_midblock2)\n","      y2.append(x)\n","      x = torch.cat(y2,dim=1)\n","\n","      x=self.leftblock4(x)\n","      y1 = self.midblock1(self.feed_to_midblock1)\n","      y1.append(x)\n","      x =torch.cat(y1,dim=1)\n","\n","      return self.lucky_layer(x)      \n","  def training_step(self, batch, batch_nb):\n","      inp_dim = self.hparams[\"input_dim\"]\n","      data,_ = batch\n","      if data.shape[0] != self.hparams[\"batch_size\"]:\n","        print(\"(train)Not full batchsize:\",data.shape[0],\"!=\",self.hparams[\"batch_size\"])\n","        A = torch.rand((1,3,32,32)).cuda()\n","        B = calculate_full_loss(A,A,hparams)-1\n","        return {\"loss\": B}\n","      batch,labels = build_batch_and_labels(data,inp_dim)\n","      temp_tensor = torch.rand((1,3,32,32)).cuda()\n","      # cumulated loss\n","      cum_loss = calculate_full_loss(temp_tensor,temp_tensor,hparams)-1\n","      for b,l in zip(batch,labels):\n","        pred = self.forward(b)\n","        loss,loss_dic = calculate_full_loss(pred,l,self.hparams)\n","        cum_loss += loss\n","      return {\"loss\": cum_loss}\n","  def training_epoch_end(self, outputs):\n","      avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n","      logs = {'train/loss': avg_loss, 'step': self.current_epoch}\n","      return {'train_avg_loss': avg_loss, 'log': logs}\n","  def validation_step(self, batch, batch_idx):\n","      inp_dim = self.hparams[\"input_dim\"]\n","      data,_ = batch\n","      if data.shape[0] != self.hparams[\"batch_size\"]:\n","        print(\"(val)Not full batchsize:\",data.shape[0],\"!=\",self.hparams[\"batch_size\"])\n","        A = torch.rand((1,3,32,32)).cuda()\n","        B = calculate_full_loss(A,A,hparams)-1\n","        return {\"val_loss\": B}\n","      batch,labels = build_batch_and_labels(data,inp_dim)\n","      temp_tensor = torch.rand((1,3,32,32)).cuda()\n","      cum_loss = calculate_full_loss(temp_tensor,temp_tensor,hparams)-1\n","      for b,l in zip(batch,labels):\n","        pred = self.forward(b)\n","        loss,loss_dic = calculate_full_loss(pred,l,self.hparams)\n","        cum_loss += loss\n","      return {\"val_loss\": cum_loss}\n","  def validation_epoch_end(self, outputs):\n","      avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","      logs = {'val/loss': avg_loss, 'step': self.current_epoch}\n","      return {'val_avg_loss': avg_loss, 'log': logs}\n","  def test_step(self, batch, batch_idx):\n","      inp_dim = self.hparams[\"input_dim\"]\n","      data,_ = batch\n","\n","      batch,labels = build_batch_and_labels(data,inp_dim)\n","      pred = self.forward(batch)\n","      loss = calculate_full_loss(pred,labels,self.hparams)\n","      loss =  l1+ l2 + l3\n","      return{\"test_loss\":loss}\n","  def test_epoch_end(self, outputs):\n","      avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n","      logs = {'test/loss': avg_loss, 'step': self.current_epoch}\n","      return {'test_avg_loss': avg_loss, 'log': logs}\n","  def prepare_data(self):\n","      len1, len2 = calculate_split(len(ufc101_dset_train),hparams[\"batch_size\"])\n","      print(len1,len2)\n","      self.train_dset, self.val_dset = torchU.data.random_split(ufc101_dset_train,[len1,len2])\n","  def train_dataloader(self):\n","      return torchU.data.DataLoader(self.train_dset,hparams[\"batch_size\"],shuffle=True,collate_fn=custom_collate,num_workers=4)\n","  def val_dataloader(self):\n","      return torchU.data.DataLoader(self.val_dset,hparams[\"batch_size\"],shuffle=False,collate_fn=custom_collate,num_workers=4)\n","  def test_dataloader(self):\n","      return torchU.data.DataLoader(self.val_dset,hparams[\"batch_size\"],shuffle=False,collate_fn=custom_collate,num_workers=4)\n","  def configure_optimizers(self):\n","      optimizer = None\n","      if self.hparams[\"optimizer\"] == 'sgd':\n","          optimizer = torch.optim.SGD(self.parameters(), lr=self.hparams[\"learning_rate\"], momentum=0.8,\n","                                      nesterov=True)\n","      elif self.hparams[\"optimizer\"] == 'adam':\n","          optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams[\"learning_rate\"])\n","      elif self.hparams[\"optimizer\"] == 'adag':\n","          optimizer = torch.optim.Adagrad(self.parameters(), lr=self.hparams[\"learning_rate\"])\n","      elif self.hparams[\"optimizer\"] == 'adad':\n","          optimizer = torch.optim.Adadelta(self.parameters(), lr=self.hparams[\"learning_rate\"])\n","      elif self.hparams[\"optimizer\"] == 'rmsp':\n","          optimizer = torch.optim.RMSprop(self.parameters(), lr=self.hparams[\"learning_rate\"], momentum=0.8)\n","      lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, 0.98)\n","      return [optimizer], [lr_scheduler]\n","  def reset_convGRU(self,batch_size):\n","    for i in range(len(self.midblock1.module_list)):\n","      if i==0:\n","        continue\n","      self.midblock1.module_list[i].init_hidden(batch_size)\n","      self.midblock2.module_list[i].init_hidden(batch_size)\n","      self.midblock3.module_list[i].init_hidden(batch_size)\n","      self.midblock4.module_list[i].init_hidden(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a1R-7JMAl7Qi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600273828832,"user_tz":-120,"elapsed":2236,"user":{"displayName":"Heiko Schmidt","photoUrl":"","userId":"06882927474289994531"}},"outputId":"2fb25854-8a4b-4ef8-83f3-9284fdbd48db"},"source":["\"\"\"\n","  prepare model specification for training\n","\"\"\"\n","epoches=1\n","torch_light.seed_everything()\n","\n","\"\"\"\n","  hparams\n","\"\"\"\n","hparams={}\n","hparams[\"optimizer\"]=\"adam\"\n","hparams[\"learning_rate\"]=0.001\n","hparams[\"batch_size\"]=16\n","batch_size=hparams[\"batch_size\"]\n","hparams[\"input_dim\"]=inp_dim\n","hparams[\"model\"]= pred_Project.load_from_checkpoint(\"model_01.ptl\")\n","hparams[\"model\"] = hparams[\"model\"].cuda()\n","hparams[\"loss_func\"]=\"crossent\"\n","hparams[\"lin_dim\"] = [pow(2,9),pow(2,8)]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["No correct seed found, seed set to 595414502\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"eQJk9oaH1ar4","colab_type":"text"},"source":["bceloss oder binarycrossentropy"]},{"cell_type":"code","metadata":{"id":"oFDXxIRbhTyv","colab_type":"code","colab":{}},"source":["class class_Project(torch_light.LightningModule):\n","  def __init__(self, hparams):\n","    super(class_Project, self).__init__()\n","    self.hparams = hparams\n","    self.trained_net = hparams[\"model\"]\n","    self.trained_net.reset_convGRU(1)\n","    self.lin_dim = hparams[\"lin_dim\"]\n","\n","    _,shapes = self.get_hidden_states()\n","    self.linear_layers = self.create_linear_layers(shapes)\n","    self.final_linear_layer = nn.Linear(self.lin_dim[1]*4*3,101)\n","  def forward(self, x):\n","    collector = []\n","    hidden_states,_ = self.get_hidden_states()\n","    for i,block in enumerate(hidden_states):\n","      for j,state in enumerate(block):\n","        collector.append((self.linear_layers[i][j])(state))\n","    collector = torch.cat(collector,dim=0)\n","    collector = collector.view(1,-1)\n","    return self.final_linear_layer(collector)\n","  def training_step(self, batch, batch_nb):\n","    pred = []\n","    labels = []\n","    for video,label in batch:\n","      self.trained_net.reset_convGRU(1)\n","      for frame in video:\n","        _ = self.trained_net(frame.unsqueeze(0))\n","      pred.append(self.forward(None))\n","      labels.append(torch.Tensor([label]))\n","    pred = torch.stack(pred)\n","    pred = pred.view(pred.shape[0],pred.shape[2])\n","    labels = torch.stack(labels)\n","    labels = labels.type(torch.LongTensor)\n","    labels = (labels.view(labels.shape[0])).cuda()\n","    loss_func = self.get_loss_func()\n","    loss = loss_func(pred,labels)\n","    acc = torch.sum(torch.max(pred, 1)[1] == labels).float() / len(labels)\n","    return {\"loss\": loss,\"train/acc\":acc}\n","  def training_epoch_end(self, outputs):\n","    avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n","    avg_acc = torch.stack([x['train/acc'] for x in outputs]).mean()\n","    logs = {'train/loss': avg_loss, 'step': self.current_epoch,'train/acc':avg_acc}\n","    return {'train_avg_loss': avg_loss, 'log': logs}\n","  def validation_step(self, batch, batch_idx):\n","    pred = []\n","    labels = []\n","    for video,label in batch:\n","      self.trained_net.reset_convGRU(1)\n","      for frame in video:\n","        _ = self.trained_net(frame.unsqueeze(0))\n","      pred.append(self.forward(None))\n","      labels.append(torch.Tensor([label]))\n","    pred = torch.stack(pred)\n","    pred = pred.view(pred.shape[0],pred.shape[2])\n","    labels = torch.stack(labels)\n","    labels = labels.type(torch.LongTensor)\n","    labels = (labels.view(labels.shape[0])).cuda()\n","    loss_func = self.get_loss_func()\n","    loss = loss_func(pred,labels)\n","    acc = torch.sum(torch.max(pred, 1)[1] == labels).float() / len(labels)\n","    return {\"val_loss\": loss,\"val/acc\":acc}\n","  def validation_epoch_end(self, outputs):\n","    avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n","    avg_acc = torch.stack([x['val/acc'] for x in outputs]).mean()\n","    logs = {'val/loss': avg_loss, 'step': self.current_epoch,'val/acc':avg_acc}\n","    return {'val_avg_loss': avg_loss, 'log': logs}\n","  def test_step(self, batch, batch_idx):\n","    pred = []\n","    labels = []\n","    for video,label in batch:\n","      self.trained_net.reset_convGRU(1)\n","      for frame in video:\n","        _ = self.trained_net(frame.unsqueeze(0))\n","      pred.append(self.forward(None))\n","      labels.append(torch.Tensor([label]))\n","    pred = torch.stack(pred)\n","    pred = pred.view(pred.shape[0],pred.shape[2])\n","    labels = torch.stack(labels)\n","    labels = labels.type(torch.LongTensor)\n","    labels = (labels.view(labels.shape[0])).cuda()\n","    loss_func = self.get_loss_func()\n","    loss = loss_func(pred,labels)\n","    acc = torch.sum(torch.max(pred, 1)[1] == labels).float() / len(labels)\n","    return {\"test_loss\": loss,\"test/acc\":acc}\n","  def test_epoch_end(self, outputs):\n","    avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n","    avg_acc = torch.stack([x['test/acc'] for x in outputs]).mean()\n","    logs = {'test/loss': avg_loss, 'step': self.current_epoch,'test/acc':avg_acc}\n","    return {'test_avg_loss': avg_loss, 'log': logs}\n","  def prepare_data(self):\n","    len1, len2,len3 = calculate_split(len(ufc101_dset_train),hparams[\"batch_size\"])\n","    print(len1,len2,len3)\n","    self.train_dset, self.val_dset,self.test_dset = torchU.data.random_split(ufc101_dset_train,[len1,len2,len3])\n","  def train_dataloader(self):\n","    return torchU.data.DataLoader(self.train_dset,hparams[\"batch_size\"],shuffle=True,collate_fn=custom_collate,num_workers=4)\n","  def val_dataloader(self):\n","    return torchU.data.DataLoader(self.val_dset,hparams[\"batch_size\"],shuffle=False,collate_fn=custom_collate,num_workers=4)\n","  def test_dataloader(self):\n","    return torchU.data.DataLoader(self.test_dset,hparams[\"batch_size\"],shuffle=False,collate_fn=custom_collate,num_workers=4)\n","  def configure_optimizers(self):\n","    optimizer = None\n","    if self.hparams[\"optimizer\"] == 'sgd':\n","        optimizer = torch.optim.SGD(self.parameters(), lr=self.hparams[\"learning_rate\"], momentum=0.8,\n","                                    nesterov=True)\n","    elif self.hparams[\"optimizer\"] == 'adam':\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams[\"learning_rate\"])\n","    elif self.hparams[\"optimizer\"] == 'adag':\n","        optimizer = torch.optim.Adagrad(self.parameters(), lr=self.hparams[\"learning_rate\"])\n","    elif self.hparams[\"optimizer\"] == 'adad':\n","        optimizer = torch.optim.Adadelta(self.parameters(), lr=self.hparams[\"learning_rate\"])\n","    elif self.hparams[\"optimizer\"] == 'rmsp':\n","        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.hparams[\"learning_rate\"], momentum=0.8)\n","    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, 0.98)\n","    return [optimizer], [lr_scheduler]\n","  def get_loss_func(self):\n","    if hparams[\"loss_func\"]==\"bce\":\n","      return nn.BCELoss()\n","    if hparams[\"loss_func\"]==\"crossent\":\n","      return nn.CrossEntropyLoss()\n","  def get_hidden_states(self):\n","    block1 = self.trained_net.midblock1.module_list[1:]\n","    block2 = self.trained_net.midblock2.module_list[1:]\n","    block3 = self.trained_net.midblock3.module_list[1:]\n","    block4 = self.trained_net.midblock4.module_list[1:]\n","\n","    hidden_states = []\n","    hidden_states1 = [x.h_cur.view(x.h_cur.shape[0],-1) for x in block1]\n","    hidden_states2 = [x.h_cur.view(x.h_cur.shape[0],-1) for x in block2]\n","    hidden_states3 = [x.h_cur.view(x.h_cur.shape[0],-1) for x in block3]\n","    hidden_states4 = [x.h_cur.view(x.h_cur.shape[0],-1) for x in block4]\n","\n","    hidden_states.append(hidden_states1)\n","    hidden_states.append(hidden_states2)\n","    hidden_states.append(hidden_states3)\n","    hidden_states.append(hidden_states4)\n","\n","    shapes = []\n","    shapes1 = [x.shape[1] for x in hidden_states1]\n","    shapes2 = [x.shape[1] for x in hidden_states2]\n","    shapes3 = [x.shape[1] for x in hidden_states3]\n","    shapes4 = [x.shape[1] for x in hidden_states4]\n","\n","    shapes.append(shapes1)\n","    shapes.append(shapes2)\n","    shapes.append(shapes3)\n","    shapes.append(shapes4)\n","    return hidden_states,shapes\n","  def create_linear_layers(self,shapes):\n","    layer_list = [[],[],[],[]]\n","    for i,block in enumerate(shapes):\n","      for j,GRU in enumerate(block):\n","        tmp = nn.Sequential(\n","            nn.Linear(GRU,self.lin_dim[0]),\n","            nn.Sigmoid(),\n","            nn.Linear(self.lin_dim[0],self.lin_dim[1])\n","        )\n","        layer_list[i].append(tmp)\n","      layer_list[i] = nn.ModuleList(layer_list[i])\n","    return nn.ModuleList(layer_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aEZ6Ggo7g_ed","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1600273836042,"user_tz":-120,"elapsed":2264,"user":{"displayName":"Heiko Schmidt","photoUrl":"","userId":"06882927474289994531"}},"outputId":"e5d4a3a2-b101-4730-d4ef-2e4593a1b2fc"},"source":["\"\"\"\n","  initialize net and trainer\n","\"\"\"\n","net_p2 = class_Project(hparams)\n","trainer= torch_light.Trainer(max_epochs=epoches,gpus=-1,fast_dev_run=False,checkpoint_callback=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","CUDA_VISIBLE_DEVICES: [0]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rVALqThRkPtE","colab":{"base_uri":"https://localhost:8080/","height":528,"referenced_widgets":["e6b99af529e34359a0b5e3f6e9d50626","bcdb8969c5dd45cea48b7cd7fc582d1b","6618cf5042ff44f1987d25726e74c2b7","1889977d151443d196da8b2093d125dc","0bdb0206be2e4f55a2dad5318f4a66ef","86fb6cab96904b09a41e9686b7cd10cb","cbb2c4e20c734057b9abf082467ade30","e7ba5686a630441488f2b0c7ee6a5cb6","0b503583aa4a4094af72faa1bb00600a","7e413ef65e1241339f9c3e828fa744c4","ebd2de4296d9460c8580b7d8abd53180","455f7602259249bda031984ab512fdc4","c372e6bec6b2493da7c29d30cd15d055","6af9c196021e46e7be6da9b82af41780","cf43a1a7d2054876bfbe414e016da892","4eaae2f537bc4edeab21e3b471a453ca"]},"outputId":"21a2b462-e03e-4b3c-bc14-9807e2fde884"},"source":["\"\"\"\n","  Run training on model\n","\"\"\"\n","trainer.fit(net_p2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n","  warnings.warn(*args, **kwargs)\n","\n","  | Name               | Type         | Params\n","----------------------------------------------------\n","0 | trained_net        | pred_Project | 38 M  \n","1 | linear_layers      | ModuleList   | 135 M \n","2 | final_linear_layer | Linear       | 310 K \n"],"name":"stderr"},{"output_type":"stream","text":["7984 2667 2669\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6b99af529e34359a0b5e3f6e9d50626","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n","  + \"follow-up version. Please use pts_unit 'sec'.\"\n","/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n","  + \"follow-up version. Please use pts_unit 'sec'.\"\n","/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n","  + \"follow-up version. Please use pts_unit 'sec'.\"\n","/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n","  + \"follow-up version. Please use pts_unit 'sec'.\"\n"],"name":"stderr"},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b503583aa4a4094af72faa1bb00600a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n","  + \"follow-up version. Please use pts_unit 'sec'.\"\n","/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n","  + \"follow-up version. Please use pts_unit 'sec'.\"\n","/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n","  + \"follow-up version. Please use pts_unit 'sec'.\"\n","/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n","  + \"follow-up version. Please use pts_unit 'sec'.\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zUges2xM8_33","colab_type":"code","colab":{}},"source":["\"\"\"\n","  Save model as checkpoint\n","\"\"\"\n","\n","trainer.save_checkpoint(\"model_c_01.ptl\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BincAI6qlJyF","colab_type":"code","colab":{}},"source":["\"\"\"\n","  Run test on model\n","\"\"\"\n","trainer.test(net_p2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"azD2ouw9sC_o","colab_type":"code","colab":{}},"source":["%reload_ext tensorboard\n","#%cd \"/content/drive/My Drive/Cuda Vision Lab Final Project\"\n","%cd \"/content\"\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D1waoqKp9ETk","colab_type":"code","colab":{}},"source":["%tensorboard --logdir lightning_logs"],"execution_count":null,"outputs":[]}]}