{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_model_1_V_tl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9fbc7f4e6c8f4871af2d91c1411f8744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8302cba658874bc180ca9a738a2ad512",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1cf5f226210344b685122071986f0db0",
              "IPY_MODEL_e580d90c0f4f4ed0a785e23fe91101ff"
            ]
          }
        },
        "8302cba658874bc180ca9a738a2ad512": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cf5f226210344b685122071986f0db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_686f2802d3aa460ba55feb63b18b8831",
            "_dom_classes": [],
            "description": " 12%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 833,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 96,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_26cc4d305ec4427fbf40bd3bcfa2653a"
          }
        },
        "e580d90c0f4f4ed0a785e23fe91101ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4556899228ab4b42b4a51656c48ae21b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 96/833 [00:32&lt;06:17,  1.95it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8bdbfac19bd644889a0d286027e5cde9"
          }
        },
        "686f2802d3aa460ba55feb63b18b8831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "26cc4d305ec4427fbf40bd3bcfa2653a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4556899228ab4b42b4a51656c48ae21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8bdbfac19bd644889a0d286027e5cde9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf0e4b2b3c3f46d49e815e05f7a73b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e5df7afdf6fc428c84f7c41c4af69303",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2e3d00006971497bbd5948595ba4c32a",
              "IPY_MODEL_615b711b1d9944b8b2604dacda0430af"
            ]
          }
        },
        "e5df7afdf6fc428c84f7c41c4af69303": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "2e3d00006971497bbd5948595ba4c32a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e4385e96ff7b419fb4d3b1d2b1cde288",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7ac5039467014b5590c2d040769bfba4"
          }
        },
        "615b711b1d9944b8b2604dacda0430af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6d86402b3d464ff187845117cb82a78b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1.0 [00:29&lt;00:00, 20.20s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6672253345c4e088ce54cfa363db4cc"
          }
        },
        "e4385e96ff7b419fb4d3b1d2b1cde288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7ac5039467014b5590c2d040769bfba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d86402b3d464ff187845117cb82a78b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6672253345c4e088ce54cfa363db4cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c50a76a2db7b4139b80f1484cf048c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_16ed738248184f6380ea1d923a9215fd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_082478bf10db4b90a32d49c597498cb8",
              "IPY_MODEL_9f9a7b750e664ba983f142bbbf88b687"
            ]
          }
        },
        "16ed738248184f6380ea1d923a9215fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "082478bf10db4b90a32d49c597498cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90a001b134a8446aa4fbe3a279309b01",
            "_dom_classes": [],
            "description": "Epoch 0:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 264,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_559325b34a5c4a25a90a95d3f85a33d6"
          }
        },
        "9f9a7b750e664ba983f142bbbf88b687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2bddc5d005d24ff2aaad4957f9abebe8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/264 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7de511dc7c114891acd5281298149099"
          }
        },
        "90a001b134a8446aa4fbe3a279309b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "559325b34a5c4a25a90a95d3f85a33d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2bddc5d005d24ff2aaad4957f9abebe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7de511dc7c114891acd5281298149099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTZcnNRKf6PW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "3f7049c7-7b04-49cd-f8fb-56be47099a71"
      },
      "source": [
        "\"\"\"\n",
        "  run and restart runtime once, av library needs that for some reason, it is need \n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "!pip install pytorch_lightning\n",
        "import torch.utils as torchU\n",
        "import pytorch_lightning as torch_light\n",
        "from torchvision import datasets, models, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "!pip install av\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.18.5)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.18.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (20.4)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.6.0+cu101)\n",
            "Requirement already satisfied: tensorboard==2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.2.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytorch_lightning) (2.4.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (3.2.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (50.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (0.4.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (0.35.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch_lightning) (1.32.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.2.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch_lightning) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch_lightning) (4.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch_lightning) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard==2.2.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.6/dist-packages (8.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XA5fB4mC0dSH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e50e99da-a0fd-4b28-b8ff-bc970c656df1"
      },
      "source": [
        "%cd \"/content/drive/My Drive/Cuda Vision Lab Final Project\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Cuda Vision Lab Final Project\n",
            "lightning_logs\t     models\t    train_model_1_V_t.ipynb\n",
            "missing_parts.ipynb  testing.ipynb  train_model_1_V_tl.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3LfEJxATIBC",
        "colab_type": "text"
      },
      "source": [
        "List of own or others modules:<br> \n",
        "own: LD_conv2d, Identity, Paraleller<br>\n",
        "others: ConvGRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtK7rvq3DhC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  copy pasted from pytorch_ssim for easier code control and ability to change code(had errors)\n",
        "\"\"\"\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([math.exp(-(x - window_size/2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size))\n",
        "    return window\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
        "    int_pad = int(window_size/2)\n",
        "    mu1 = F.conv2d(img1, window, padding = int_pad, groups = channel)\n",
        "    mu2 = F.conv2d(img2, window, padding = int_pad, groups = channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1*mu2\n",
        "\n",
        "    int_pad = int(window_size/2)\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding = int_pad, groups = channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding = int_pad, groups = channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding = int_pad, groups = channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "class SSIM(torch.nn.Module):\n",
        "    def __init__(self, window_size = 11, size_average = True):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = 1\n",
        "        self.window = create_window(window_size, self.channel)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        (_, channel, _, _) = img1.size()\n",
        "\n",
        "        if channel == self.channel:\n",
        "            window = self.window\n",
        "        else:\n",
        "            window = create_window(self.window_size, channel)\n",
        "            self.window = window\n",
        "            self.channel = channel\n",
        "\n",
        "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
        "\n",
        "def ssim(img1, img2, window_size = 11, size_average = True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    window = create_window(window_size, channel)\n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtQvcJ0P7ogS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  code is taken from following Git-repository:\n",
        "  https://github.com/happyjin/ConvGRU-pytorch.git\n",
        "  ConvLSTM_pytorch is released under the MIT License (refer to the LICENSE file for details).\n",
        "\"\"\"\n",
        "class ConvGRU(nn.Module):\n",
        "  def __init__(self, input_size, input_dim, hidden_dim, kernel_size, bias, dtype, batch_size):\n",
        "    \"\"\"\n",
        "    Initialize the ConvLSTM cell\n",
        "    :param input_size: (int, int)\n",
        "        Height and width of input tensor as (height, width).\n",
        "    :param input_dim: int\n",
        "        Number of channels of input tensor.\n",
        "    :param hidden_dim: int\n",
        "        Number of channels of hidden state.\n",
        "    :param kernel_size: (int, int)\n",
        "        Size of the convolutional kernel.\n",
        "    :param bias: bool\n",
        "        Whether or not to add the bias.\n",
        "    :param dtype: torch.cuda.FloatTensor or torch.FloatTensor\n",
        "        Whether or not to use cuda.\n",
        "    \"\"\"\n",
        "    super(ConvGRU, self).__init__()\n",
        "    self.height, self.width = input_size\n",
        "    self.padding = kernel_size // 2, kernel_size // 2\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.bias = bias\n",
        "    self.dtype = dtype\n",
        "    self.init_hidden(batch_size)\n",
        "\n",
        "    self.conv_gates = nn.Conv2d(in_channels=input_dim + hidden_dim,\n",
        "                                out_channels=2*self.hidden_dim,  # for update_gate,reset_gate respectively\n",
        "                                kernel_size=kernel_size,\n",
        "                                padding=self.padding,\n",
        "                                bias=self.bias)\n",
        "\n",
        "    self.conv_can = nn.Conv2d(in_channels=input_dim+hidden_dim,\n",
        "                          out_channels=self.hidden_dim, # for candidate neural memory\n",
        "                          kernel_size=kernel_size,\n",
        "                          padding=self.padding,\n",
        "                          bias=self.bias)\n",
        "  def init_hidden(self, batch_size):\n",
        "    self.h_cur = (Variable(torch.zeros(batch_size, self.hidden_dim, self.height, self.width)).type(self.dtype))\n",
        "  def forward(self, input_tensor):\n",
        "    \"\"\"\n",
        "    :param self:\n",
        "    :param input_tensor: (b, c, h, w)\n",
        "        input is actually the target_model\n",
        "    :param h_cur: (b, c_hidden, h, w)\n",
        "        current hidden and cell states respectively\n",
        "    :return: h_next,\n",
        "        next hidden state\n",
        "    \"\"\"\n",
        "    self.check_device(input_tensor)\n",
        "    combined = torch.cat([input_tensor, self.h_cur], dim=1)\n",
        "    combined_conv = self.conv_gates(combined)\n",
        "\n",
        "    gamma, beta = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "    reset_gate = torch.sigmoid(gamma)\n",
        "    update_gate = torch.sigmoid(beta)\n",
        "\n",
        "    combined = torch.cat([input_tensor, reset_gate*self.h_cur], dim=1)\n",
        "    cc_cnm = self.conv_can(combined)\n",
        "    cnm = torch.tanh(cc_cnm)\n",
        "\n",
        "    h_next = (1 - update_gate) * self.h_cur + update_gate * cnm\n",
        "    return h_next\n",
        "  def check_device(self,input_tensors):\n",
        "    if self.h_cur.device != input_tensors.device:\n",
        "      self.h_cur.data = self.h_cur.to(input_tensors.get_device())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si9pGdQXUgx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  test code for convGRU\n",
        "\"\"\"\n",
        "convGru = ConvGRU((32,32),64,128,3,bias=False,dtype=torch.FloatTensor,batch_size=128)\n",
        "A = torch.rand((128,64,32,32))\n",
        "B = convGru(A)\n",
        "print(B.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfbAMCqxZ3gW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  This module is provided from our supervisor Hafez Farazi\n",
        "  related paper: http://ais.uni-bonn.de/~hfarazi/papers/LocDep.pdf\n",
        "  code also provided from: https://github.com/AIS-Bonn/LocDepVideoPrediction.git\n",
        "\"\"\"\n",
        "class LocationAwareConv2d(torch.nn.Conv2d):\n",
        "    def __init__(self,w,h,in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
        "        super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.locationBias=torch.nn.Parameter(torch.zeros(w,h,3))\n",
        "        self.locationEncode=torch.autograd.Variable(torch.ones(w,h,3))\n",
        "    def forward(self,inputs):\n",
        "        if self.locationBias.device != inputs.device:\n",
        "            self.locationBias.data=self.locationBias.to(inputs.get_device())\n",
        "        if self.locationEncode.device != inputs.device:\n",
        "            self.locationEncode.data=self.locationEncode.to(inputs.get_device())\n",
        "        b=self.locationBias*self.locationEncode\n",
        "        return super().forward(inputs)+b[:,:,0]+b[:,:,1]+b[:,:,2]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-k9NYUsbJLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  test code for LocationAwareConv2d\n",
        "\"\"\"\n",
        "convLoc = LocationAwareConv2d(w=32,h=32,in_channels=64,out_channels=64,kernel_size=1)\n",
        "A = torch.rand((32,64,32,32))\n",
        "B = convLoc(A)\n",
        "print(B.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glXn3MtlTGC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  module to deal with midblocks of model 1\n",
        "\"\"\"\n",
        "class Paraleller(nn.Module):\n",
        "  def __init__(self,list_of_modules):\n",
        "    super(Paraleller, self).__init__()\n",
        "    self.module_list = nn.ModuleList(list_of_modules)\n",
        "  def forward(self, x):\n",
        "    ans = []\n",
        "    for module in self.module_list:\n",
        "      ans.append(module(x))\n",
        "    return ans"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoEJDqhymsUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd \"/content\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB2BBtQZSAUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  download the dataset and annonations\n",
        "\"\"\"\n",
        "!wget https://www.crcv.ucf.edu/data/UCF101/UCF101.rar --no-check-certificate\n",
        "!wget https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip --no-check-certificate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt_n7hEqSIQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  unpack downloaded dataset and annnonations\n",
        "\"\"\"\n",
        "get_ipython().system_raw(\"unrar x UCF101.rar\")\n",
        "!unzip UCF101TrainTestSplits-RecognitionTask.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7YWq4t37glp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# important constant: (frames_per_clip-3)*batchsize = true batchsize\n",
        "frames_per_clip = 6\n",
        "frames_between_clip = 60\n",
        "inp_dim=128"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KYKI5pU-GzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfs = transforms.Compose([\n",
        "             # scale in [0, 1]\n",
        "             transforms.Lambda(lambda x: x / 255.),\n",
        "             # reshape into (T, C, H, W)\n",
        "             transforms.Lambda(lambda x: x.permute(0, 3, 1, 2) ) ,\n",
        "             # resize frames\n",
        "             transforms.Lambda(lambda x: F.interpolate(x,(inp_dim,inp_dim)))\n",
        "    ])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IKwV1Z0gZOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9fbc7f4e6c8f4871af2d91c1411f8744",
            "8302cba658874bc180ca9a738a2ad512",
            "1cf5f226210344b685122071986f0db0",
            "e580d90c0f4f4ed0a785e23fe91101ff",
            "686f2802d3aa460ba55feb63b18b8831",
            "26cc4d305ec4427fbf40bd3bcfa2653a",
            "4556899228ab4b42b4a51656c48ae21b",
            "8bdbfac19bd644889a0d286027e5cde9"
          ]
        },
        "outputId": "8e4e2c59-e529-49fa-99fb-3c9affe0c481"
      },
      "source": [
        "\"\"\"\n",
        "  create the dataset to \"train\" the model\n",
        "\"\"\"\n",
        "ufc101_dset_train = datasets.UCF101(root=\"UCF-101\",\n",
        "                                    annotation_path=\"ucfTrainTestlist\", \n",
        "                                    frames_per_clip=frames_per_clip,\n",
        "                                    step_between_clips=frames_between_clip,\n",
        "                                    train=True,\n",
        "                                    transform=tfs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fbc7f4e6c8f4871af2d91c1411f8744",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=833.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0bw4R-z79XU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c54e8d1a-dea6-42b3-c557-fb9159912787"
      },
      "source": [
        "print(len(ufc101_dset_train))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhTWpvMNfMA7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "bd9ee724-97c7-4e9c-c691-fa0b0b5096b8"
      },
      "source": [
        "loader = torchU.data.DataLoader(ufc101_dset_train,batch_size=10,collate_fn=custom_collate)\n",
        "rnd_number = int(torch.randint(10,size=(1,)))\n",
        "print(rnd_number)\n",
        "counter=1\n",
        "chosen_batch = None\n",
        "chosen_labels = None\n",
        "for batch,label in loader:\n",
        "  if counter==rnd_number:\n",
        "    chosen_batch,chosen_labels = build_batch_and_labels(batch,inp_dim)\n",
        "    break\n",
        "  counter+=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 3, 3, 64, 64]) torch.Size([10, 3, 3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-croSRyfwVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "56845d85-9557-49cd-d741-111b7a4e0335"
      },
      "source": [
        "for b_ele,l_ele in zip(chosen_batch,chosen_labels):\n",
        "  print(b_ele.shape,l_ele.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 3, 64, 64]) torch.Size([10, 3, 32, 32])\n",
            "torch.Size([10, 3, 64, 64]) torch.Size([10, 3, 32, 32])\n",
            "torch.Size([10, 3, 64, 64]) torch.Size([10, 3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK38iNU8lnyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_batch_and_labels(data,inp_dim):\n",
        "  batch = data[:,:data.shape[1]-3,:,:,:]\n",
        "  labels = data[:,3:,:,:,:]\n",
        "  labels = F.interpolate(labels,(3,inp_dim//2,inp_dim//2))\n",
        "  batches = []\n",
        "  labeles = []\n",
        "  for i in range(batch.shape[1]):\n",
        "    if torch.cuda.is_available():\n",
        "      batches.append(batch[:,i,:,:,:].cuda())\n",
        "      labeles.append(labels[:,i,:,:,:].cuda())\n",
        "    else:\n",
        "      batches.append(batch[:,i,:,:,:])\n",
        "      labeles.append(labels[:,i,:,:,:])\n",
        "  return batches,labeles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CdkuOcEwm8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_collate(batch):\n",
        "    filtered_batch = []\n",
        "    for video, _, label in batch:\n",
        "        filtered_batch.append((video, label))\n",
        "    return torch.utils.data.dataloader.default_collate(filtered_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SU7IoiFZ6akl",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  loss function wrapper\n",
        "\"\"\"\n",
        "def calculate_full_loss(pred,labels,hparams):\n",
        "  loss_func_l1 = nn.L1Loss()\n",
        "  loss_func_l2 = nn.MSELoss()\n",
        "  loss_func_ssim = SSIM(window_size=1)\n",
        "  l1 = hparams[\"loss_scale\"][0]*(1-loss_func_ssim(pred.cpu(),labels.cpu())/2)\n",
        "  l2 = hparams[\"loss_scale\"][1]*loss_func_l2(pred,labels)\n",
        "  l3 = hparams[\"loss_scale\"][2]*loss_func_l1(pred,labels)\n",
        "  loss_dic = {\n",
        "      \"MSE\":l2,\n",
        "      \"L1\":l3,\n",
        "      \"DSSIM\":l1\n",
        "  }\n",
        "  return l1+ l2 + l3,loss_dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1R-7JMAl7Qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  prepare model specification for training\n",
        "\"\"\"\n",
        "epoches=5\n",
        "torch_light.seed_everything()\n",
        "\n",
        "\"\"\"\n",
        "  hparams\n",
        "\"\"\"\n",
        "hparams={}\n",
        "hparams[\"optimizer\"]=\"adam\"\n",
        "hparams[\"learning_rate\"]=0.001\n",
        "hparams[\"batch_size\"]=128\n",
        "batch_size=hparams[\"batch_size\"]\n",
        "hparams[\"input_dim\"]=inp_dim\n",
        "hparams[\"fp_clip\"]=frames_per_clip\n",
        "hparams[\"loss_scale\"]=[1.0,0.7,0.2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeF7qisomNru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_split(dataset_len,batch_size):\n",
        "  len1 = int(dataset_len*0.7)\n",
        "  while len1%batch_size!=0:\n",
        "    len1-=len1%batch_size\n",
        "  len2 = dataset_len-len1\n",
        "  return len1,len2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGH3wW26nKDh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "da0ce83b-bec3-45df-8581-aeebd6628817"
      },
      "source": [
        "len1,len2 = calculate_split(len(ufc101_dset_train),batch_size)\n",
        "print(len1,len2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22656 9720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw1njHSN3zEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = torch.rand((1,3,32,32)).cuda()\n",
        "B = calculate_full_loss(A,A,hparams)-1\n",
        "print(type(B),B.shape,B)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh7nJQTk9MRY",
        "colab_type": "text"
      },
      "source": [
        "convGru = ConvGRU((32,32),64,128,3,bias=False,dtype=torch.FloatTensor,batchsize=128)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFDXxIRbhTyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class pred_Project(torch_light.LightningModule):\n",
        "  def __init__(self, hparams):\n",
        "    super(pred_Project, self).__init__()\n",
        "    self.hparams = hparams\n",
        "    \n",
        "    \"\"\"\n",
        "      load pretrained resnet18\n",
        "    \"\"\"\n",
        "    self.resnet = models.resnet18(pretrained=True)\n",
        "    \n",
        "    \"\"\"\n",
        "      some constants of the network\n",
        "    \"\"\"\n",
        "    inp_size = self.hparams[\"input_dim\"] #assumption number is even and power of 2\n",
        "    i2 = int(inp_size/2)\n",
        "    i4 = int(inp_size/4)\n",
        "    i8 = int(inp_size/8)\n",
        "    i16 = int(inp_size/16)\n",
        "    true_batchsize = self.hparams[\"batch_size\"]\n",
        "    \"\"\"\n",
        "      create a partial layer of the proposed network and structure them in packs\n",
        "    \"\"\"\n",
        "    #BN+Relu+Maxpool\n",
        "    self.pre_block = nn.Sequential(\n",
        "        self.resnet.bn1,\n",
        "        self.resnet.relu,\n",
        "        self.resnet.maxpool\n",
        "    )\n",
        "    #middle row block 1\n",
        "    self.midblock1 = Paraleller(\n",
        "        [nn.Conv2d(64,64,1,1),\n",
        "          ConvGRU((i2,i2),64,64,kernel_size=3,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n",
        "          ConvGRU((i2,i2),64,64,kernel_size=5,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n",
        "          ConvGRU((i2,i2),64,64,kernel_size=7,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize)  \n",
        "        ]\n",
        "    )\n",
        "    #middle row block 2\n",
        "    self.midblock2 = Paraleller(\n",
        "        [nn.Conv2d(64,64,1,1),\n",
        "          ConvGRU((i4,i4),64,64,kernel_size=3,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n",
        "          ConvGRU((i4,i4),64,64,kernel_size=5,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n",
        "          ConvGRU((i4,i4),64,64,kernel_size=7,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize)\n",
        "        ]\n",
        "    )\n",
        "    #middle row block 3\n",
        "    self.midblock3 = Paraleller(\n",
        "        [LocationAwareConv2d(w=i8,h=i8,in_channels=128,out_channels=64,kernel_size=1),\n",
        "          ConvGRU((i8,i8),128,64,kernel_size=3,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n",
        "          ConvGRU((i8,i8),128,64,kernel_size=5,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n",
        "          ConvGRU((i8,i8),128,64,kernel_size=7,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize)\n",
        "        ]\n",
        "    )\n",
        "    #middle row block 4\n",
        "    self.midblock4 = Paraleller(\n",
        "        [LocationAwareConv2d(w=i16,h=i16,in_channels=256,out_channels=64,kernel_size=1),\n",
        "          ConvGRU((i16,i16),256,64,kernel_size=3,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n",
        "          ConvGRU((i16,i16),256,64,kernel_size=5,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize),\n",
        "          ConvGRU((i16,i16),256,64,kernel_size=7,bias=False,dtype=torch.FloatTensor,batch_size=true_batchsize)\n",
        "        ]\n",
        "    )\n",
        "    #left block block 1\n",
        "    self.leftblock1 = nn.Sequential(\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(512,1024,3,1,1),\n",
        "        nn.PixelShuffle(2),\n",
        "        nn.Conv2d(256,64,3,1,1)\n",
        "    )\n",
        "    #left block block 2\n",
        "    self.leftblock2 = nn.Sequential(\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(320,1024,3,1,1),\n",
        "        nn.PixelShuffle(2),\n",
        "        nn.Conv2d(256,64,3,1,1)\n",
        "    )\n",
        "    #left block block 3\n",
        "    self.leftblock3 = nn.Sequential(\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(320,1024,3,1,1),\n",
        "        nn.PixelShuffle(2),\n",
        "        nn.Conv2d(256,64,3,1,1)\n",
        "    )\n",
        "    #left block block 4\n",
        "    self.leftblock4 = nn.Sequential(\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(320,1024,3,1,1),\n",
        "        nn.PixelShuffle(2),\n",
        "        nn.Conv2d(256,64,3,1,1)\n",
        "    )\n",
        "    # last layer\n",
        "    self.lucky_layer = nn.Conv2d(320,3,1,1,0)\n",
        "  def forward(self, x):\n",
        "    x = self.resnet.conv1(x)\n",
        "    self.feed_to_midblock1 = x.clone()\n",
        "    x = self.pre_block(x)\n",
        "    x = self.resnet.layer1(x)\n",
        "    self.feed_to_midblock2 = x.clone()\n",
        "    x = self.resnet.layer2(x)\n",
        "    self.feed_to_midblock3 = x.clone()\n",
        "    x = self.resnet.layer3(x)\n",
        "    self.feed_to_midblock4 = x.clone()\n",
        "    x = self.resnet.layer4(x)\n",
        "    \n",
        "    x = self.leftblock1(x)\n",
        "    y4 = self.midblock4(self.feed_to_midblock4)\n",
        "    y4.append(x)\n",
        "    x = torch.cat(y4,dim=1)\n",
        "\n",
        "    x = self.leftblock2(x)\n",
        "    y3 = self.midblock3(self.feed_to_midblock3)\n",
        "    y3.append(x)\n",
        "    x = torch.cat(y3,dim=1)\n",
        "\n",
        "    x=self.leftblock3(x)\n",
        "    y2 = self.midblock2(self.feed_to_midblock2)\n",
        "    y2.append(x)\n",
        "    x = torch.cat(y2,dim=1)\n",
        "\n",
        "    x=self.leftblock4(x)\n",
        "    y1 = self.midblock1(self.feed_to_midblock1)\n",
        "    y1.append(x)\n",
        "    x =torch.cat(y1,dim=1)\n",
        "\n",
        "    return self.lucky_layer(x)      \n",
        "  def training_step(self, batch, batch_nb):\n",
        "    self.reset_convGRU(self.hparams[\"batch_size\"])\n",
        "    inp_dim = self.hparams[\"input_dim\"]\n",
        "    data,_ = batch\n",
        "    if data.shape[0] != self.hparams[\"batch_size\"]:\n",
        "      print(\"(train)Not full batchsize:\",data.shape[0],\"!=\",self.hparams[\"batch_size\"])\n",
        "      A = torch.rand((1,3,32,32)).cuda()\n",
        "      B = calculate_full_loss(A,A,hparams)-1\n",
        "      return {\"loss\": B}\n",
        "    batch,labels = build_batch_and_labels(data,inp_dim)\n",
        "    temp_tensor = torch.rand((1,3,32,32)).cuda()\n",
        "    # cumulated loss\n",
        "    loss,loss_dic = calculate_full_loss(temp_tensor,temp_tensor,hparams)\n",
        "    cum_loss = loss-1\n",
        "    for b,l in zip(batch,labels):\n",
        "      pred = self.forward(b)\n",
        "      loss,loss_dic = calculate_full_loss(pred,l,self.hparams)\n",
        "      cum_loss += loss\n",
        "    return {\"loss\": cum_loss}\n",
        "  def training_epoch_end(self, outputs):\n",
        "    avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "    logs = {'train/loss': avg_loss, 'step': self.current_epoch}\n",
        "    return {'train_avg_loss': avg_loss, 'log': logs}\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    self.reset_convGRU(self.hparams[\"batch_size\"])\n",
        "    inp_dim = self.hparams[\"input_dim\"]\n",
        "    data,_ = batch\n",
        "    if data.shape[0] != self.hparams[\"batch_size\"]:\n",
        "      print(\"(val)Not full batchsize:\",data.shape[0],\"!=\",self.hparams[\"batch_size\"])\n",
        "      A = torch.rand((1,3,32,32)).cuda()\n",
        "      B = calculate_full_loss(A,A,self.hparams)-1\n",
        "      return {\"val_loss\": B}\n",
        "    batch,labels = build_batch_and_labels(data,inp_dim)\n",
        "    temp_tensor = torch.rand((1,3,32,32)).cuda()\n",
        "    loss,loss_dic = calculate_full_loss(temp_tensor,temp_tensor,hparams)\n",
        "    cum_loss = loss-1\n",
        "    for b,l in zip(batch,labels):\n",
        "      pred = self.forward(b)\n",
        "      loss,loss_dic = calculate_full_loss(pred,l,self.hparams)\n",
        "      cum_loss += loss\n",
        "    return {\"val_loss\": cum_loss}\n",
        "  def validation_epoch_end(self, outputs):\n",
        "    avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "    logs = {'val/loss': avg_loss, 'step': self.current_epoch}\n",
        "    return {'val_avg_loss': avg_loss, 'log': logs}\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    self.reset_convGRU(self.hparams[\"batch_size\"])\n",
        "    inp_dim = self.hparams[\"input_dim\"]\n",
        "    data,_ = batch\n",
        "    if data.shape[0] != self.hparams[\"batch_size\"]:\n",
        "      print(\"(test)Not full batchsize:\",data.shape[0],\"!=\",self.hparams[\"batch_size\"])\n",
        "      A = torch.rand((1,3,32,32)).cuda()\n",
        "      B = calculate_full_loss(A,A,self.hparams)-1\n",
        "      return {\"test_loss\": B}\n",
        "    batch,labels = build_batch_and_labels(data,inp_dim)\n",
        "    temp_tensor = torch.rand((1,3,32,32)).cuda()\n",
        "    loss,loss_dic = calculate_full_loss(temp_tensor,temp_tensor,hparams)\n",
        "    cum_loss = loss-1\n",
        "    for b,l in zip(batch,labels):\n",
        "      pred = self.forward(b)\n",
        "      loss,loss_dic = calculate_full_loss(pred,l,self.hparams)\n",
        "      cum_loss += loss\n",
        "    return{\"test_loss\":loss}\n",
        "  def test_epoch_end(self, outputs):\n",
        "    avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
        "    logs = {'test/loss': avg_loss, 'step': self.current_epoch}\n",
        "    return {'test_avg_loss': avg_loss, 'log': logs}\n",
        "  def prepare_data(self):\n",
        "    len1, len2 = calculate_split(len(ufc101_dset_train),hparams[\"batch_size\"])\n",
        "    print(len1,len2)\n",
        "    self.train_dset, self.val_dset = torchU.data.random_split(ufc101_dset_train,[len1,len2])\n",
        "  def train_dataloader(self):\n",
        "    return torchU.data.DataLoader(self.train_dset,hparams[\"batch_size\"],shuffle=True,collate_fn=custom_collate,num_workers=4)\n",
        "  def val_dataloader(self):\n",
        "    return torchU.data.DataLoader(self.val_dset,hparams[\"batch_size\"],shuffle=False,collate_fn=custom_collate,num_workers=4)\n",
        "  def test_dataloader(self):\n",
        "    return torchU.data.DataLoader(self.val_dset,hparams[\"batch_size\"],shuffle=False,collate_fn=custom_collate,num_workers=4)\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = None\n",
        "    if self.hparams[\"optimizer\"] == 'sgd':\n",
        "        optimizer = torch.optim.SGD(self.parameters(), lr=self.hparams[\"learning_rate\"], momentum=0.8,\n",
        "                                    nesterov=True)\n",
        "    elif self.hparams[\"optimizer\"] == 'adam':\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams[\"learning_rate\"])\n",
        "    elif self.hparams[\"optimizer\"] == 'adag':\n",
        "        optimizer = torch.optim.Adagrad(self.parameters(), lr=self.hparams[\"learning_rate\"])\n",
        "    elif self.hparams[\"optimizer\"] == 'adad':\n",
        "        optimizer = torch.optim.Adadelta(self.parameters(), lr=self.hparams[\"learning_rate\"])\n",
        "    elif self.hparams[\"optimizer\"] == 'rmsp':\n",
        "        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.hparams[\"learning_rate\"], momentum=0.8)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, 0.98)\n",
        "    return [optimizer], [lr_scheduler]\n",
        "  def reset_convGRU(self,batch_size):\n",
        "    for i in range(len(self.midblock1.module_list)):\n",
        "      if i==0:\n",
        "        continue\n",
        "      self.midblock1.module_list[i].init_hidden(batch_size)\n",
        "      self.midblock2.module_list[i].init_hidden(batch_size)\n",
        "      self.midblock3.module_list[i].init_hidden(batch_size)\n",
        "      self.midblock4.module_list[i].init_hidden(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEZ6Ggo7g_ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  initialize net and trainer\n",
        "\"\"\"\n",
        "net_p1 = pred_Project(hparams)\n",
        "trainer= torch_light.Trainer(max_epochs=epoches,gpus=-1,fast_dev_run=False,checkpoint_callback=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rVALqThRkPtE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bf0e4b2b3c3f46d49e815e05f7a73b93",
            "e5df7afdf6fc428c84f7c41c4af69303",
            "2e3d00006971497bbd5948595ba4c32a",
            "615b711b1d9944b8b2604dacda0430af",
            "e4385e96ff7b419fb4d3b1d2b1cde288",
            "7ac5039467014b5590c2d040769bfba4",
            "6d86402b3d464ff187845117cb82a78b",
            "b6672253345c4e088ce54cfa363db4cc",
            "c50a76a2db7b4139b80f1484cf048c1c",
            "16ed738248184f6380ea1d923a9215fd",
            "082478bf10db4b90a32d49c597498cb8",
            "9f9a7b750e664ba983f142bbbf88b687",
            "90a001b134a8446aa4fbe3a279309b01",
            "559325b34a5c4a25a90a95d3f85a33d6",
            "2bddc5d005d24ff2aaad4957f9abebe8",
            "7de511dc7c114891acd5281298149099"
          ]
        },
        "outputId": "28e5848c-b6b5-425a-fdf8-279d08331dad"
      },
      "source": [
        "\"\"\"\n",
        "  Run training on model\n",
        "\"\"\"\n",
        "trainer.fit(net_p1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "   | Name        | Type       | Params\n",
            "--------------------------------------------\n",
            "0  | resnet      | ResNet     | 11 M  \n",
            "1  | pre_block   | Sequential | 128   \n",
            "2  | midblock1   | Paraleller | 2 M   \n",
            "3  | midblock2   | Paraleller | 2 M   \n",
            "4  | midblock3   | Paraleller | 3 M   \n",
            "5  | midblock4   | Paraleller | 5 M   \n",
            "6  | leftblock1  | Sequential | 4 M   \n",
            "7  | leftblock2  | Sequential | 3 M   \n",
            "8  | leftblock3  | Sequential | 3 M   \n",
            "9  | leftblock4  | Sequential | 3 M   \n",
            "10 | lucky_layer | Conv2d     | 963   \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "23552 10178\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf0e4b2b3c3f46d49e815e05f7a73b93",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c50a76a2db7b4139b80f1484cf048c1c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-59c6989134be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mRun\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mon\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_p1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/states.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mentering\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPUBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/gpu_backend.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;31m# TRAINING_STEP + TRAINING_STEP_END\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;31m# ------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m# only track outputs when user implements training_epoch_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    842\u001b[0m                     \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m                 )\n\u001b[1;32m    846\u001b[0m                 \u001b[0musing_results_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_closure_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36moptimizer_closure\u001b[0;34m(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 training_step_output = self.training_forward(split_batch, batch_idx, opt_idx,\n\u001b[0;32m-> 1015\u001b[0;31m                                                              hiddens)\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtraining_forward\u001b[0;34m(self, batch, batch_idx, opt_idx, hiddens)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransfer_batch_to_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0;31m# TPU support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-c9da6c0d6811>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mcum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_full_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0mcum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-c9da6c0d6811>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleftblock4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmidblock1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_to_midblock1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3a2f4c78e59e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-373e6ccc180b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_gate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_cur\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mcc_cnm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_can\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mcnm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc_cnm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mh_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mupdate_gate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_cur\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mupdate_gate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcnm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 14.73 GiB total capacity; 13.42 GiB already allocated; 21.88 MiB free; 13.71 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUges2xM8_33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Save model as checkpoint\n",
        "\"\"\"\n",
        "\n",
        "trainer.save_checkpoint(\"model_02.ptl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjVo6bMT9drM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Load model as checkpoint\n",
        "\"\"\"\n",
        "\n",
        "net_p1 = pred_Project.load_from_checkpoint(\"model_01.ptl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BincAI6qlJyF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Run test on model\n",
        "\"\"\"\n",
        "trainer.test(net_p1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUzpaKPY_6Bt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(inp,title=None,normalize = 0):\n",
        "    inp = torchvision.utils.make_grid(inp)\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    plt.figure(figsize = (30,20))\n",
        "    if normalize==1:\n",
        "      inp = (inp +1)/2\n",
        "    elif normalize==2:\n",
        "      pass\n",
        "    inp = inp.permute((1, 2, 0))\n",
        "    plt.imshow(inp.detach().cpu())\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh_P8ZgGjY8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_tensor(AA):\n",
        "  batch_size,c,h,w = AA.shape\n",
        "  AA = AA.view(AA.size(0), -1)\n",
        "  AA -= AA.min(1, keepdim=True)[0]\n",
        "  AA /= AA.max(1, keepdim=True)[0]\n",
        "  AA = AA.view(batch_size,c, h, w)\n",
        "  return AA"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3k6FdqWme2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale_tensor(inp,size):\n",
        "  new_size = (int(size),int(size))\n",
        "  return F.interpolate(\n",
        "      input = inp,\n",
        "      size=new_size,\n",
        "      mode='bilinear'\n",
        "  )"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOL05ZENlOf2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "8b38d37e-9c55-4421-ec45-ea46540ea5b8"
      },
      "source": [
        "\"\"\"\n",
        "  Test the quality of our model for the 3 seed frames + 3 predicted frames\n",
        "\"\"\"\n",
        "#net_p1 = net_p1.cuda()\n",
        "loader = torchU.data.DataLoader(ufc101_dset_train,batch_size=1,collate_fn=custom_collate)\n",
        "rnd_number = int(torch.randint(100,size=(1,)))\n",
        "print(rnd_number)\n",
        "counter=1\n",
        "chosen_batch = None\n",
        "chosen_labels = None\n",
        "for batch,label in loader:\n",
        "  if counter==rnd_number:\n",
        "    chosen_batch,chosen_labels = build_batch_and_labels(batch,inp_dim)\n",
        "    break\n",
        "  counter+=1\n",
        "batch = torch.cat(chosen_batch,dim=0)\n",
        "labels = torch.cat(chosen_labels,dim=0)\n",
        "tmp = scale_tensor(batch,inp_dim//2)\n",
        "# # truth\n",
        "truth_frames = torch.cat([tmp,labels],dim=0)\n",
        "net_p1.reset_convGRU(1)\n",
        "pred_frames = []\n",
        "for i in range(len(batch)):\n",
        "  pred_frames.append(net_p1(batch[i].view((1,) + batch[i].shape)))\n",
        "pred_frames = torch.cat(pred_frames,dim=0)\n",
        "# # prediction\n",
        "pred_frames = torch.cat([tmp,pred_frames],dim=0)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/io/video.py:106: UserWarning: The pts_unit 'pts' gives wrong results and will be removed in a follow-up version. Please use pts_unit 'sec'.\n",
            "  + \"follow-up version. Please use pts_unit 'sec'.\"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRFaJQvI7Ymz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "e093f3ce-17c8-4bdd-ac5a-33f1c2d6c925"
      },
      "source": [
        "# cell to plot upsampled truth vs prediction\n",
        "truth = truth_frames.clone()\n",
        "pred = pred_frames.clone()\n",
        "#upsampling\n",
        "sc = 4\n",
        "truth = scale_tensor(truth,truth.shape[2]*sc)\n",
        "pred = scale_tensor(pred,pred.shape[2]*sc)\n",
        "truth = normalize_tensor(truth)\n",
        "pred = normalize_tensor(pred)\n",
        "\n",
        "truth_pred = torch.cat([truth,pred],dim=0)\n",
        "# plot truth and pred as images\n",
        "truth_pred_g = torchvision.utils.make_grid(truth_pred,nrow=6)\n",
        "# imshow(truth_pred_g)\n",
        "torchvision.utils.save_image(truth_pred_g,\"truth_pred.png\")\n",
        "\n",
        "# plot model and truth as gif\n",
        "loop = 300\n",
        "create_gif(\"truth.gif\",truth,loop)\n",
        "create_gif(\"pred.gif\",pred,loop)\n",
        "\n",
        "# zip the results for easier download\n",
        "!zip results.zip truth.gif pred.gif truth_pred.png\n",
        "!rm truth.gif pred.gif truth_pred.png"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  adding: truth.gif (deflated 38%)\n",
            "  adding: pred.gif (deflated 40%)\n",
            "  adding: truth_pred.png (deflated 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BA4yuDonKyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_gif(filename,images,duration):\n",
        "  tf = transforms.ToPILImage()\n",
        "  tmp_l = []\n",
        "  for img in images:\n",
        "    tmp_l.append(tf(img))\n",
        "  tmp_l[0].save(filename,\n",
        "               save_all=True,\n",
        "               append_images=tmp_l[1:],\n",
        "               duration=duration,\n",
        "               loop=0)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azD2ouw9sC_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext tensorboard\n",
        "#%cd \"/content/drive/My Drive/Cuda Vision Lab Final Project\"\n",
        "%cd \"/content\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OYrPkJtaTD2p",
        "colab": {}
      },
      "source": [
        "%cd \"/content\"\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1waoqKp9ETk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir lightning_logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}